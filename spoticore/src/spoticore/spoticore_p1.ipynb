{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spoticore *Stage 1*\n",
    "\n",
    "Evolution from the shallow bigram nueral network architecture to a complete Multiple Layer Perceptron that handles character level context lengths greater than 1.\n",
    "\n",
    "Based on [Bengio et al., 2003](https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf)\n",
    "\n",
    "## Contents\n",
    "\n",
    "1. **Setup & Data Processing** - Load constants, data readers, and vocabulary builders\n",
    "2. **Model Architecture** - MLP implementation with embedding, hidden, and output layers\n",
    "3. **Training** - Forward/backward passes and optimization\n",
    "4. **Evaluation** - Development set validation\n",
    "5. **Text Generation** - Sample from the trained model\n",
    "6. **Experiments** - Learning rate tuning and architecture exploration\n",
    "\n",
    "## Model Overview\n",
    "\n",
    "- **Input**: Fixed-length character sequences (context windows)\n",
    "- **Architecture**: Embedding → Hidden Layer (tanh) → Output Layer (softmax)\n",
    "- **Task**: Predict the next character given context\n",
    "- **Training**: Cross-entropy loss with mini-batch SGD\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Constants\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Seed.\n",
    "SAMPLE_SEED: Final[int] = 534150593\n",
    "\n",
    "# Data Processing.\n",
    "LYRICS_COLUMN: Final[str] = \"text\"\n",
    "DEFAULT_CSV_PATH: Final[str] = \"spotify_lyrics.csv\"\n",
    "\n",
    "# Model Architecture - MLP.\n",
    "BLOCK_SIZE: int = 3  # Context window size (number of characters).\n",
    "\n",
    "# Training Hyperparameters.\n",
    "LEARNING_RATE: float = 0.1\n",
    "REGULARIZATION_FACTOR: float = 0.001\n",
    "EMBEDDING_DIM: int = 10  # Dimension of character embeddings.\n",
    "HIDDEN_LAYER_SIZE: int = 100  # Number of neurons in the hidden layer.\n",
    "BATCH_SIZE: int = 32  # Number of inputs per training iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Reader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_all_lyrics(csv_path: str = DEFAULT_CSV_PATH) -> list[str]:\n",
    "    \"\"\"\n",
    "    Process the Spotify lyrics CSV file and extract all lyrics text.\n",
    "\n",
    "    Args:\n",
    "        csv_path: Path to the CSV file. Defaults to \"spotify_lyrics.csv\".\n",
    "\n",
    "    Returns:\n",
    "        List of lyrics text strings, one per song.\n",
    "    \"\"\"\n",
    "    lyrics_list = []\n",
    "\n",
    "    with open(csv_path, \"r\", encoding=\"utf-8\") as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "\n",
    "        for row in reader:\n",
    "            # Extract the lyrics text from the 'text' column.\n",
    "            lyrics = row.get(LYRICS_COLUMN, \"\").strip().lower()\n",
    "            if lyrics:  # Only add non-empty lyrics.\n",
    "                lyrics_list.append(lyrics)\n",
    "\n",
    "    return lyrics_list\n",
    "\n",
    "\n",
    "def read_all_unique_words(csv_path: str = DEFAULT_CSV_PATH) -> list[str]:\n",
    "    \"\"\"\n",
    "    Process the Spotify lyrics CSV file and extract all unique individual words.\n",
    "\n",
    "    Punctuation is removed from words before adding them to the vocabulary.\n",
    "\n",
    "    Args:\n",
    "        csv_path: Path to the CSV file. Defaults to \"spotify_lyrics.csv\".\n",
    "\n",
    "    Returns:\n",
    "        Sorted list of unique words from all lyrics, with punctuation removed.\n",
    "    \"\"\"\n",
    "    words_list = []\n",
    "    # Create a translation table that replaces punctuation with spaces.\n",
    "    translator = str.maketrans({punct: \" \" for punct in string.punctuation})\n",
    "\n",
    "    with open(csv_path, \"r\", encoding=\"utf-8\") as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        for row in reader:\n",
    "            lyrics = row.get(LYRICS_COLUMN, \"\").strip().lower()\n",
    "            if lyrics:\n",
    "                # Replace punctuation with spaces before splitting.\n",
    "                sanitized_lyrics = lyrics.translate(translator)\n",
    "                # Filter out empty strings created by consecutive spaces.\n",
    "                words_list.extend([word for word in sanitized_lyrics.split() if word])\n",
    "\n",
    "    return sorted(set(words_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. MLP Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass(frozen=True)\n",
    "class Parameters:\n",
    "    \"\"\"\n",
    "    Container for MLP network parameters.\n",
    "\n",
    "    Holds all trainable weights and biases for the three-layer network:\n",
    "    - Embedding layer (C)\n",
    "    - Hidden layer (W1, b1)\n",
    "    - Output layer (W2, b2)\n",
    "\n",
    "    Attributes:\n",
    "        C: Embedding matrix of shape (vocab_size, embedding_dim).\n",
    "           Maps character indices to dense vector representations.\n",
    "        W1: Hidden layer weight matrix of shape (block_size * embedding_dim, hidden_size).\n",
    "        b1: Hidden layer bias vector of shape (hidden_size,).\n",
    "        W2: Output layer weight matrix of shape (hidden_size, vocab_size).\n",
    "        b2: Output layer bias vector of shape (vocab_size,).\n",
    "    \"\"\"\n",
    "\n",
    "    C: torch.Tensor\n",
    "    W1: torch.Tensor\n",
    "    W2: torch.Tensor\n",
    "    b1: torch.Tensor\n",
    "    b2: torch.Tensor\n",
    "\n",
    "    @property\n",
    "    def parameters(self) -> tuple[torch.Tensor, ...]:\n",
    "        \"\"\"Return all parameters as a tuple for gradient operations.\"\"\"\n",
    "        return (self.C, self.W1, self.W2, self.b1, self.b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab_from_words():\n",
    "    \"\"\"\n",
    "    Build vocabulary mappings from unique words.\n",
    "\n",
    "    Reads all unique words from the CSV file and creates string-to-index (stoi)\n",
    "    and index-to-string (itos) mappings for all characters found in the words.\n",
    "    The period character (.) is assigned index 0.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing:\n",
    "            - stoi (dict[str, int]): Mapping from character to index.\n",
    "            - itos (dict[int, str]): Mapping from index to character.\n",
    "            - words (list[str]): List of all unique words.\n",
    "    \"\"\"\n",
    "    words = read_all_unique_words()\n",
    "    chars = sorted(list(set(\".\".join(words))))\n",
    "    stoi = {char: i for i, char in enumerate(chars)}\n",
    "    stoi[\".\"] = 0\n",
    "    itos = {i: char for i, char in enumerate(stoi)}\n",
    "\n",
    "    return stoi, itos, words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset(\n",
    "    words: list[str],\n",
    "    stoi: dict[str, int],\n",
    "    block_size: int = BLOCK_SIZE,\n",
    ") -> tuple[torch.Tensor, torch.Tensor, int]:\n",
    "    \"\"\"\n",
    "    Build training dataset from words using character-level context windows.\n",
    "\n",
    "    Creates input-output pairs where each input is a context window of characters\n",
    "    and the output is the next character to predict. Uses a sliding window approach\n",
    "    with a fixed context length (block_size).\n",
    "\n",
    "    Args:\n",
    "        words: List of words to build the dataset from.\n",
    "        stoi: Mapping from character to index for converting characters to tensor inputs.\n",
    "        block_size: Number of characters in the context window. Defaults to 3.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing:\n",
    "            - X (torch.Tensor): Input tensor of shape (n_samples, block_size) with context windows.\n",
    "            - Y (torch.Tensor): Output tensor of shape (n_samples,) with target character indices.\n",
    "            - block_size (int): The context window size used.\n",
    "    \"\"\"\n",
    "    # Input and corresponding output (labels) matrices.\n",
    "    X, Y = [], []\n",
    "\n",
    "    for word in words:\n",
    "        prev_char_idxs = [0] * block_size\n",
    "        for char in word + \".\":\n",
    "            next_char_idx = stoi[char]\n",
    "            X.append(prev_char_idxs)\n",
    "            Y.append(next_char_idx)\n",
    "            prev_char_idxs = prev_char_idxs[1:] + [next_char_idx]\n",
    "\n",
    "    return torch.tensor(X), torch.tensor(Y), block_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Splitting\n",
    "\n",
    "Split the data into three disjoint sets:\n",
    "- **Training (80%)**: Used to update model weights via gradient descent\n",
    "- **Development/Validation (10%)**: Used to tune hyperparameters and monitor overfitting\n",
    "- **Testing (10%)**: Held out for final performance evaluation (used only once)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass(frozen=True)\n",
    "class SplitData:\n",
    "    \"\"\"\n",
    "    Container for train/dev/test dataset splits.\n",
    "\n",
    "    Holds all data needed for training, validation, and testing with\n",
    "    convenient property accessors for each split.\n",
    "\n",
    "    Attributes:\n",
    "        vocab_size: Number of unique characters in the vocabulary.\n",
    "        X_train: Training input tensor of shape (n_train_samples, block_size).\n",
    "        X_dev: Development input tensor of shape (n_dev_samples, block_size).\n",
    "        X_test: Test input tensor of shape (n_test_samples, block_size).\n",
    "        Y_train: Training target tensor of shape (n_train_samples,).\n",
    "        Y_dev: Development target tensor of shape (n_dev_samples,).\n",
    "        Y_test: Test target tensor of shape (n_test_samples,).\n",
    "        ctxlen_train: Context window size for training (should equal BLOCK_SIZE).\n",
    "        ctxlen_dev: Context window size for development (should equal BLOCK_SIZE).\n",
    "        ctxlen_test: Context window size for testing (should equal BLOCK_SIZE).\n",
    "    \"\"\"\n",
    "\n",
    "    vocab_size: int = 0\n",
    "\n",
    "    X_train: torch.Tensor = torch.tensor(0)\n",
    "    X_dev: torch.Tensor = torch.tensor(0)\n",
    "    X_test: torch.Tensor = torch.tensor(0)\n",
    "\n",
    "    Y_train: torch.Tensor = torch.tensor(0)\n",
    "    Y_dev: torch.Tensor = torch.tensor(0)\n",
    "    Y_test: torch.Tensor = torch.tensor(0)\n",
    "\n",
    "    ctxlen_train: int = 0\n",
    "    ctxlen_dev: int = 0\n",
    "    ctxlen_test: int = 0\n",
    "\n",
    "    @property\n",
    "    def train_data(self) -> tuple[torch.Tensor, torch.Tensor, int]:\n",
    "        \"\"\"Return (X_train, Y_train, context_length) tuple.\"\"\"\n",
    "        return (self.X_train, self.Y_train, self.ctxlen_train)\n",
    "\n",
    "    @property\n",
    "    def dev_data(self) -> tuple[torch.Tensor, torch.Tensor, int]:\n",
    "        \"\"\"Return (X_dev, Y_dev, context_length) tuple.\"\"\"\n",
    "        return (self.X_dev, self.Y_dev, self.ctxlen_dev)\n",
    "\n",
    "    @property\n",
    "    def test_data(self) -> tuple[torch.Tensor, torch.Tensor, int]:\n",
    "        \"\"\"Return (X_test, Y_test, context_length) tuple.\"\"\"\n",
    "        return (self.X_test, self.Y_test, self.ctxlen_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_training_dataset():\n",
    "    \"\"\"\n",
    "    Split the dataset into training, development, and test sets.\n",
    "\n",
    "    Loads all unique words from the vocabulary, shuffles them deterministically,\n",
    "    and splits into three disjoint sets:\n",
    "    - Training (80%): For optimizing model weights via gradient descent\n",
    "    - Development (10%): For tuning hyperparameters and detecting overfitting\n",
    "    - Testing (10%): For final unbiased performance evaluation\n",
    "\n",
    "    Returns:\n",
    "        SplitData: Object containing:\n",
    "            - vocab_size: Number of unique characters\n",
    "            - X_train, Y_train: Training input/output tensors\n",
    "            - X_dev, Y_dev: Development input/output tensors\n",
    "            - X_test, Y_test: Testing input/output tensors\n",
    "            - ctxlen_train/dev/test: Context window sizes (should all be equal)\n",
    "\n",
    "    Note:\n",
    "        Uses fixed random seed (42) for reproducible splits across runs.\n",
    "        Each word is expanded into multiple (context, next_char) training examples.\n",
    "    \"\"\"\n",
    "    random.seed(42)\n",
    "\n",
    "    stoi, _, words = build_vocab_from_words()\n",
    "\n",
    "    random.shuffle(words)\n",
    "\n",
    "    n1 = int(0.8 * len(words))\n",
    "    n2 = int(0.9 * len(words))\n",
    "\n",
    "    X_train, Y_train, train_ctxlen = build_dataset(words[:n1], stoi)\n",
    "    X_dev, Y_dev, dev_ctxlen = build_dataset(words[n1:n2], stoi)\n",
    "    X_test, Y_test, test_ctxlen = build_dataset(words[n2:], stoi)\n",
    "\n",
    "    return SplitData(\n",
    "        len(stoi),\n",
    "        X_train,\n",
    "        X_dev,\n",
    "        X_test,\n",
    "        Y_train,\n",
    "        Y_dev,\n",
    "        Y_test,\n",
    "        train_ctxlen,\n",
    "        dev_ctxlen,\n",
    "        test_ctxlen,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(\n",
    "    vocab_size: int, block_size: int, generator: torch.Generator | None = None\n",
    ") -> Parameters:\n",
    "    \"\"\"\n",
    "    Initialize network parameters with random values.\n",
    "\n",
    "    Uses standard normal initialization for now. Each input to the network\n",
    "    is flattened into a row vector of size (BLOCK_SIZE * EMBEDDING_DIM).\n",
    "\n",
    "    Args:\n",
    "        vocab_size: Size of the vocabulary (number of unique characters).\n",
    "        block_size: Number of characters in the context window.\n",
    "        generator: Optional random number generator for reproducible initialization.\n",
    "\n",
    "    Returns:\n",
    "        Parameters: Container with initialized embedding, weight, and bias tensors.\n",
    "    \"\"\"\n",
    "    if generator is None:\n",
    "        generator = torch.Generator().manual_seed(SAMPLE_SEED)\n",
    "\n",
    "    # Embedding layer weights.\n",
    "    C = torch.randn(\n",
    "        (vocab_size, EMBEDDING_DIM), generator=generator, requires_grad=True\n",
    "    )\n",
    "\n",
    "    # Hidden layer weights and bias.\n",
    "    # Each row of W1 holds the weights from one input feature (1 of 30) to all hidden neurons.\n",
    "    # Each column holds the weights from all input features (all 30) to one hidden neuron.\n",
    "    W1 = torch.randn(\n",
    "        (block_size * EMBEDDING_DIM, HIDDEN_LAYER_SIZE),\n",
    "        generator=generator,\n",
    "        requires_grad=True,\n",
    "    )\n",
    "    b1 = torch.randn(HIDDEN_LAYER_SIZE, generator=generator, requires_grad=True)\n",
    "\n",
    "    # Output layer weights and bias.\n",
    "    W2 = torch.randn(\n",
    "        (HIDDEN_LAYER_SIZE, vocab_size), generator=generator, requires_grad=True\n",
    "    )\n",
    "    b2 = torch.randn(vocab_size, generator=generator, requires_grad=True)\n",
    "\n",
    "    return Parameters(C, W1, W2, b1, b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(\n",
    "    X: torch.Tensor,\n",
    "    Y: torch.Tensor,\n",
    "    parameters: Parameters,\n",
    "    block_size: int,\n",
    ") -> tuple[Parameters, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Perform forward pass through the MLP network for character-level language modeling.\n",
    "\n",
    "    The network consists of:\n",
    "    1. Embedding layer: Converts character indices to dense embeddings.\n",
    "    2. Hidden layer: Fully connected layer with tanh activation.\n",
    "    3. Output layer: Produces logits for next character prediction.\n",
    "\n",
    "    Args:\n",
    "        X: Input tensor of shape (n_samples, block_size) containing character indices.\n",
    "        Y: Target tensor of shape (n_samples,) containing the true next character indices.\n",
    "        parameters: Container with all network parameters (embedding weights, W1, W2, b1, b2).\n",
    "        block_size: Number of characters in the context window.\n",
    "\n",
    "    Returns:\n",
    "        Tuple containing:\n",
    "            - Parameters: Container with the parameter tensors (same reference as input).\n",
    "            - loss: The cross entropy loss tensor.\n",
    "    \"\"\"\n",
    "    emb_dims = EMBEDDING_DIM\n",
    "\n",
    "    emb = parameters.C[X]  # [num_samples, block_size, emb_dims]\n",
    "    num_samples = emb.shape[0]\n",
    "\n",
    "    # Flatten emb tensor to 2d for matrix multiplication with weight matrix.\n",
    "    # One input sample contains 3 characters and each character is embedded as a vector of size 10.\n",
    "    # Each sample becomes a single 30 element vector containing all the context information.\n",
    "    emb = emb.view(num_samples, block_size * emb_dims)\n",
    "\n",
    "    # Each row of W1 corresponds to one of the 30 input features (after flattening) of an input sample.\n",
    "    # Each hidden neuron receives a weighted sum of all 30 features.\n",
    "    h = torch.tanh(emb @ parameters.W1 + parameters.b1)  # [num_samples, hidden_size]\n",
    "\n",
    "    # Output layer contains a node for each character that comes next; i.e. vocab_size neurons.\n",
    "    logits = h @ parameters.W2 + parameters.b2  # [num_samples, vocab_size]\n",
    "    loss = F.cross_entropy(logits, Y)\n",
    "\n",
    "    return parameters, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_pass(\n",
    "    parameters: list[torch.Tensor],\n",
    "    loss: torch.Tensor,\n",
    "    lr: torch.Tensor | float = LEARNING_RATE,\n",
    "):\n",
    "    \"\"\"\n",
    "    Perform backward pass and update parameters using gradient descent.\n",
    "\n",
    "    Computes gradients via backpropagation and updates all network parameters\n",
    "    in-place using the update rule: param = param - lr * gradient.\n",
    "\n",
    "    Args:\n",
    "        parameters: List of parameter tensors to update (must have requires_grad=True).\n",
    "        loss: The computed loss tensor from forward pass.\n",
    "        lr: Learning rate for gradient descent. Can be a float or tensor for dynamic LR.\n",
    "            Defaults to LEARNING_RATE constant.\n",
    "\n",
    "    Returns:\n",
    "        None. Parameters are updated in-place.\n",
    "\n",
    "    Note:\n",
    "        - Zeros out all gradients before backward pass to avoid accumulation\n",
    "        - Raises warning if any parameter has no gradient\n",
    "    \"\"\"\n",
    "\n",
    "    # Just in case.\n",
    "    for p in parameters:\n",
    "        p.requires_grad = True\n",
    "\n",
    "    # Compute gradients via backpropagation.\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    # Update parameters using gradient descent.\n",
    "    for p in parameters:\n",
    "        if p.grad is not None:\n",
    "            p.data -= lr * p.grad\n",
    "        else:\n",
    "            # This shouldn't happen.\n",
    "            print(\n",
    "                f\"Warning: Parameter with shape {p.shape} has no gradient (requires_grad={p.requires_grad})\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.2  # from experiment 1 below\n",
    "\n",
    "\n",
    "def train(\n",
    "    num_iterations: int = 1000,\n",
    "    print_interval: int = 10,\n",
    "    generator: torch.Generator | None = None,\n",
    ") -> tuple[Parameters, SplitData]:\n",
    "    \"\"\"\n",
    "    Train the MLP network for character-level language modeling.\n",
    "\n",
    "    Performs the complete training process: data preparation, parameter initialization,\n",
    "    and training loop with forward and backward passes with input batches.\n",
    "\n",
    "    Args:\n",
    "        num_iterations: Number of training iterations (default: 1000).\n",
    "        print_interval: Print loss every N iterations (default: 10).\n",
    "        generator: Optional random number generator. If None, creates one with SAMPLE_SEED.\n",
    "\n",
    "    Returns:\n",
    "        params: The updated parameters after training.\n",
    "        data:  The dataset split used for training/dev/test.\n",
    "    \"\"\"\n",
    "    # Initialize random number generator with fixed seed for reproducibility.\n",
    "    if generator is None:\n",
    "        generator = torch.Generator().manual_seed(SAMPLE_SEED)\n",
    "\n",
    "    # Build split training dataset from all words.\n",
    "    data = split_training_dataset()\n",
    "\n",
    "    # extract training input and outputs\n",
    "    X, Y, block_size = data.train_data\n",
    "\n",
    "    # Initialize network parameters based on the training dataset\n",
    "    params = initialize_parameters(data.vocab_size, block_size, generator=generator)\n",
    "    print(\"Parameters initialized\")\n",
    "\n",
    "    n_params = sum(p.nelement() for p in params.parameters)\n",
    "    print(\"Total paramters:\", n_params)\n",
    "\n",
    "    # Training loop.\n",
    "    for i in range(num_iterations):\n",
    "        # mini batch construct\n",
    "        ix = torch.randint(0, X.shape[0], (BATCH_SIZE,), generator=generator)\n",
    "\n",
    "        # extract the batches\n",
    "        X_batch, Y_batch = X[ix], Y[ix]\n",
    "\n",
    "        # Forward pass: compute predictions and loss with the mini batch of inputs\n",
    "        params, loss = forward_pass(X_batch, Y_batch, params, block_size)\n",
    "\n",
    "        # Print loss at specified intervals.\n",
    "        if i % print_interval == 0:\n",
    "            print(f\"Iteration {i}: loss = {loss.item():.4f}\")\n",
    "\n",
    "        # Backward pass: compute gradients and update parameters.\n",
    "        backward_pass(list(params.parameters), loss)\n",
    "\n",
    "    print(f\"Training complete. Final loss: {loss.item():.4f}\")\n",
    "\n",
    "    return params, data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.Generator().manual_seed(SAMPLE_SEED)\n",
    "\n",
    "params, data = train(200_000, generator=g, print_interval=10_000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation (Dev) Loss\n",
    "\n",
    "After each training run or at checkpoint intervals, we measure the loss ***without updating the weights*** on **unseen** data inputs to see how well the model generalizes to unseen data.\n",
    "\n",
    "We use this loss to fine-tune the ``hyperparameters`` of the model: learning rate, number and size of hidden layers, regularization rate, embedding dimension size etc.\n",
    "\n",
    "We can use dev loss to decide when to stop training based on the following observations:\n",
    "\n",
    "1. A significant gap between training loss and dev loss indicates **overfitting**, meaning the model has memorized the training data but does not generalize well to new data. \n",
    "\n",
    "2. If both losses are high, the model is **underfitting** and needs more capacity or training.\n",
    "\n",
    "3. If both losses are close and small, the model is **generalizing well** and we have found a good balance between fitting the training data and generalizing to unseen data. At this point, we can stop training.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Compare dev loss with trained loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_dev_loss(\n",
    "    split_data: SplitData,\n",
    "    trained_params: Parameters,\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Compute the development (validation) loss without updating weights.\n",
    "\n",
    "    Evaluates the trained model on the held-out development set to measure\n",
    "    generalization performance. Uses torch.no_grad() to disable gradient\n",
    "    computation for efficiency and to prevent accidental weight updates.\n",
    "\n",
    "    Args:\n",
    "        split_data: SplitData object containing train/dev/test splits.\n",
    "        trained_params: Parameters object with trained model weights.\n",
    "\n",
    "    Returns:\n",
    "        float: The cross-entropy loss on the development set.\n",
    "\n",
    "    Note:\n",
    "        This function prints the dev loss to stdout and returns the scalar value.\n",
    "        Compare with training loss to detect overfitting/underfitting.\n",
    "    \"\"\"\n",
    "    X_dev, Y_dev, block_size = split_data.dev_data\n",
    "\n",
    "    with torch.no_grad():\n",
    "        _, loss = forward_pass(X_dev, Y_dev, trained_params, block_size)\n",
    "\n",
    "    print(f\"Dev loss: {loss.item()}\")\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_loss = compute_dev_loss(data, params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Text Generation (Sampling)\n",
    "\n",
    "Generate new text by sampling from the trained model one character at a time.\n",
    "\n",
    "Process:\n",
    "1. Start with a context of padding characters (index 0)\n",
    "2. Feed context through the network to get probability distribution over next characters\n",
    "3. Sample a character from this distribution\n",
    "4. Update context by appending the sampled character and removing the oldest\n",
    "5. Repeat until we sample the end-of-word token (.)\n",
    "\n",
    "The quality of generated text reflects how well the model learned character-level patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_from_model(\n",
    "    trained_params: Parameters, count: int, block_size: int, generator: torch.Generator\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate text samples from the trained character-level language model.\n",
    "\n",
    "    Args:\n",
    "        trained_params: Parameters object containing trained model weights.\n",
    "        count: Number of words to generate.\n",
    "        block_size: Context window size (number of characters).\n",
    "        generator: Torch random number generator for reproducible sampling.\n",
    "\n",
    "    Returns:\n",
    "        None. Prints generated words to stdout, one per line.\n",
    "\n",
    "    Note:\n",
    "        Generated text quality depends on model training and hyperparameters.\n",
    "        Expects vocabulary to have '.' at index 0 as the end-of-word token.\n",
    "    \"\"\"\n",
    "    C, W1, b1, W2, b2 = (\n",
    "        trained_params.C,\n",
    "        trained_params.W1,\n",
    "        trained_params.b1,\n",
    "        trained_params.W2,\n",
    "        trained_params.b2,\n",
    "    )\n",
    "\n",
    "    _, itos, _ = build_vocab_from_words()\n",
    "    for _ in range(count):\n",
    "        out = []\n",
    "        context = [0] * block_size\n",
    "        while True:\n",
    "            emb = C[torch.tensor([context])]  # [1, block_size, emb_dims]\n",
    "            h = torch.tanh(emb.view(1, -1) @ W1 + b1)\n",
    "            logits = h @ W2 + b2\n",
    "            probs = F.softmax(logits, dim=1)\n",
    "            next_idx = torch.multinomial(probs, 1, True, generator=generator).item()\n",
    "            context = context[1:] + [next_idx]\n",
    "            out.append(next_idx)\n",
    "            if next_idx == 0:\n",
    "                break\n",
    "\n",
    "        print(\"\".join(itos[i] for i in out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.Generator().manual_seed(SAMPLE_SEED + 10)\n",
    "\n",
    "examples = 50\n",
    "\n",
    "sample_from_model(params, examples, BLOCK_SIZE, g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 1: Finding the Optimal Learning Rate\n",
    "\n",
    "This experiment uses a **learning rate range test** to identify the best initial learning rate for training.\n",
    "\n",
    "Method:\n",
    "- Start with a small learning rate (10⁻³ or 10⁻²) and gradually increase it to 1.0\n",
    "- Train for a fixed number of iterations, updating the learning rate at each step\n",
    "- Plot loss vs learning rate to find the region where loss decreases fastest\n",
    "\n",
    "Goal:\n",
    "Find the learning rate that:\n",
    "1. Decreases loss quickly (steep negative slope)\n",
    "2. Remains stable (doesn't cause divergence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterations = 30_000\n",
    "\n",
    "# Build vocabulary and dataset.\n",
    "stoi, itos, words = build_vocab_from_words()\n",
    "vocab_size = len(stoi)\n",
    "print(f\"Vocabulary size: {vocab_size}\")\n",
    "\n",
    "# Build training dataset from all words.\n",
    "X, Y, block_size = build_dataset(words, stoi)\n",
    "print(f\"Dataset shape: X={X.shape}, Y={Y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss vs learning rate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = torch.Generator().manual_seed(SAMPLE_SEED)\n",
    "\n",
    "# Initialize network parameters.\n",
    "params = initialize_parameters(vocab_size, block_size, generator=generator)\n",
    "\n",
    "n_params = sum(p.nelement() for p in params.parameters)\n",
    "print(\"Total paramters:\", n_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create learning rate exponents from -2 to 0 with 1000 points\n",
    "lre = torch.linspace(-2, 0, iterations)\n",
    "# Convert exponents to actual learning rates (10^lre)\n",
    "lrs = 10**lre\n",
    "\n",
    "# Initialize lists to track learning rates and losses during training\n",
    "lr_x = []\n",
    "loss_y = []\n",
    "\n",
    "batch_generator = torch.Generator().manual_seed(SAMPLE_SEED)\n",
    "\n",
    "print_interval = 10_000\n",
    "\n",
    "# Training loop.\n",
    "for i in range(iterations):\n",
    "    # mini batch construct\n",
    "    ix = torch.randint(0, X.shape[0], (BATCH_SIZE,), generator=batch_generator)\n",
    "\n",
    "    # extract the batches\n",
    "    X_batch, Y_batch = X[ix], Y[ix]\n",
    "\n",
    "    # Forward pass: compute predictions and loss with the mini batch of inputs\n",
    "    params, loss = forward_pass(X_batch, Y_batch, params, block_size)\n",
    "\n",
    "    lr = lrs[i]\n",
    "\n",
    "    # Print loss at specified intervals.\n",
    "    if i % print_interval == 0:\n",
    "        print(f\"Iteration {i}: loss = {loss.item():.4f}, lr = {lr:.6f}\")\n",
    "\n",
    "    # Backward pass: compute gradients and update parameters.\n",
    "    backward_pass(list(params.parameters), loss, lr)\n",
    "\n",
    "    # track stats\n",
    "    lr_x.append(lr.item())\n",
    "    loss_y.append(loss.item())\n",
    "\n",
    "\n",
    "print(f\"Training complete. Final loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(lr_x, loss_y)\n",
    "plt.xlabel(\"Learning Rate\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Loss vs Learning Rate\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss vs learning rate exponent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-initialize weights so Experiment 1b starts from the same point.\n",
    "generator = torch.Generator().manual_seed(SAMPLE_SEED)\n",
    "\n",
    "# Initialize network parameters.\n",
    "params = initialize_parameters(vocab_size, block_size, generator=generator)\n",
    "\n",
    "n_params = sum(p.nelement() for p in params.parameters)\n",
    "print(\"Total paramters:\", n_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create learning rate exponents from -3 to 0\n",
    "lre = torch.linspace(-3, 0, iterations)\n",
    "# Convert exponents to actual learning rates (10^exponent)\n",
    "lrs = 10**lre\n",
    "\n",
    "\n",
    "# Initialize lists to track learning rates and losses for plotting\n",
    "lr_x = []\n",
    "loss_y = []\n",
    "\n",
    "batch_generator = torch.Generator().manual_seed(SAMPLE_SEED)\n",
    "\n",
    "# Training loop.\n",
    "for i in range(iterations):\n",
    "    # mini batch construct\n",
    "    ix = torch.randint(0, X.shape[0], (BATCH_SIZE,), generator=batch_generator)\n",
    "\n",
    "    # extract the batches\n",
    "    X_batch, Y_batch = X[ix], Y[ix]\n",
    "\n",
    "    # Forward pass: compute predictions and loss with the mini batch of inputs\n",
    "    params, loss = forward_pass(X_batch, Y_batch, params, block_size)\n",
    "\n",
    "    lr = lrs[i]\n",
    "\n",
    "    # Print loss at specified intervals.\n",
    "    if i % print_interval == 0:\n",
    "        print(f\"Iteration {i}: loss = {loss.item():.4f}, lr = {lr:.6f}\")\n",
    "\n",
    "    # Backward pass: compute gradients and update parameters.\n",
    "    backward_pass(list(params.parameters), loss, lr)\n",
    "\n",
    "    # track stats\n",
    "    lr_x.append(lre[i])\n",
    "    loss_y.append(loss.item())\n",
    "\n",
    "\n",
    "print(f\"Training complete. Final loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(lr_x, loss_y)\n",
    "plt.xlabel(\"Learning Rate Exponent\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Loss vs Learning Rate Exponent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 2: Impact of Hidden Layer Size on Training\n",
    "\n",
    "This experiment investigates how the number of neurons in the hidden layer affects model performance.\n",
    "\n",
    "Method:\n",
    "- Train multiple models with different `HIDDEN_LAYER_SIZE` values (e.g., 100, 200, 300, 500)\n",
    "- Use the optimal learning rate from Experiment 1\n",
    "- Compare training loss curves to see convergence speed and final loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment configuration.\n",
    "HIDDEN_LAYER_SIZE = 500  # Test different values: 100, 200, 300, 500.\n",
    "LEARNING_RATE = 0.2  # Optimal LR from Experiment 1.\n",
    "\n",
    "# Build vocabulary and dataset.\n",
    "stoi, itos, words = build_vocab_from_words()\n",
    "vocab_size = len(stoi)\n",
    "print(f\"Vocabulary size: {vocab_size}\")\n",
    "\n",
    "# Build training dataset from all words.\n",
    "X, Y, block_size = build_dataset(words, stoi)\n",
    "print(f\"Dataset shape: X={X.shape}, Y={Y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = torch.Generator().manual_seed(SAMPLE_SEED)\n",
    "\n",
    "# Initialize network parameters.\n",
    "params = initialize_parameters(vocab_size, block_size, generator=generator)\n",
    "\n",
    "n_params = sum(p.nelement() for p in params.parameters)\n",
    "print(\"Total paramters:\", n_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_x = []\n",
    "loss_y = []\n",
    "\n",
    "batch_generator = torch.Generator().manual_seed(SAMPLE_SEED)\n",
    "\n",
    "# Training loop.\n",
    "for i in range(30_000):\n",
    "    # construct mini batches of size BATCH_SIZE\n",
    "    ix = torch.randint(0, X.shape[0], (BATCH_SIZE,), generator=batch_generator)\n",
    "\n",
    "    # extract the batches\n",
    "    X_batch, Y_batch = X[ix], Y[ix]\n",
    "\n",
    "    # Forward pass: compute predictions and loss with the mini batch of inputs\n",
    "    params, loss = forward_pass(X_batch, Y_batch, params, block_size)\n",
    "\n",
    "    # Print loss at specified intervals.\n",
    "    if i % print_interval == 0:\n",
    "        print(f\"Iteration {i:5d}: loss = {loss.item():.4f}\")\n",
    "\n",
    "    # Backward pass: compute gradients and update parameters.\n",
    "    backward_pass(list(params.parameters), loss, LEARNING_RATE)\n",
    "\n",
    "    # track stats\n",
    "    step_x.append(i)\n",
    "    loss_y.append(loss.item())\n",
    "\n",
    "\n",
    "print(f\"Training complete. Final loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(step_x, loss_y)\n",
    "plt.xlabel(\"Iteration Number\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Loss vs Iterations\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
